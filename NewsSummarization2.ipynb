{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsSummarization2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAxkQDdKUmfswtezTOPMFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mooncrater31/Competitive-Programming/blob/master/NewsSummarization2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLSt6nY9Edfv",
        "colab_type": "code",
        "outputId": "c3244e15-e4eb-45de-8cb1-bd9e056cb944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SFfqTtBkm0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf744817-a475-44a8-83e3-d437bf982ece"
      },
      "source": [
        "from os import listdir\n",
        "import string \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a document into news story and highlights\n",
        "def split_story(doc):\n",
        "\t# find first highlight\n",
        "\tindex = doc.find('@highlight')\n",
        "\t# split into story and highlights\n",
        "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
        "\t# strip extra white space around each highlight\n",
        "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
        "\treturn story, highlights\n",
        "\n",
        "# load all stories in a directory\n",
        "def load_stories(directory):\n",
        "\tstories = list()\n",
        "\tfor name in listdir(directory):\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\t# load document\n",
        "\t\tdoc = load_doc(filename)\n",
        "\t\t# split into story and highlights\n",
        "\t\tstory, highlights = split_story(doc)\n",
        "\t\t# store\n",
        "\t\tstories.append({'story':story, 'highlights':highlights})\n",
        "\treturn stories\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare a translation table to remove punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# strip source cnn office if it exists\n",
        "\t\tindex = line.find('(CNN) -- ')\n",
        "\t\tif index > -1:\n",
        "\t\t\tline = line[index+len('(CNN)'):]\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tline = [w.translate(table) for w in line]\n",
        "\t\t# remove tokens with numbers in them\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\t# remove empty strings\n",
        "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
        "\treturn cleaned\n",
        "# load stories\n",
        "directory = 'cnn/stories/'\n",
        "stories = load_stories(directory)\n",
        "print('Loaded Stories %d' % len(stories))\n",
        "for example in stories:\n",
        "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
        "\texample['highlights'] = clean_lines(example['highlights'])\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Stories 92579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfYM_bVRtO1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc7ccaeb-831d-468a-f3c7-e59f3475abd8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re \n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text  import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\",200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "stories2 = []\n",
        "for st in stories:\n",
        "  a = dict()\n",
        "  a['story'] = ''\n",
        "  for s in st['story']:\n",
        "    a['story'] += s+' '\n",
        "  a['story'] = a['story'].strip()\n",
        "  a['highlight'] = ''\n",
        "  for h in st['highlights']:\n",
        "    a['highlight']+= h+' '\n",
        "  a['highlight'] = a['highlight'].strip()\n",
        "  stories2.append(dict(a))\n",
        "for st in stories2:\n",
        "  st['highlight'] = '_START_'+st['highlight']+'_END_'\n",
        "max_len_text = 700\n",
        "max_len_summary = 70\n",
        "data = dict() \n",
        "data['cleaned_text'] = []\n",
        "data['cleaned_summary'] = []\n",
        "for s in stories2:\n",
        "  data['cleaned_text'].append(s['story'])\n",
        "  data['cleaned_summary'].append(s['highlight'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val = train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAvoSMjOMt45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
        "x_tr = pad_sequences(x_tr,maxlen=max_len_text,padding='post')\n",
        "x_val = pad_sequences(x_val,maxlen=max_len_text,padding='post')\n",
        "x_voc_size = len(x_tokenizer.word_index)+1\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "y_tr = pad_sequences(y_tr,maxlen=max_len_summary,padding='post')\n",
        "y_val = pad_sequences(y_val,maxlen=max_len_summary,padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf_s_31dM_LV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b24526fa-f65c-426f-a61d-89e15ef1518b"
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "latent_dim = 500\n",
        "\n",
        "encoder_inputs  = Input(shape=(max_len_text,))\n",
        "enc_emb = Embedding(x_voc_size,latent_dim,trainable=True)(encoder_inputs)\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "encoder_output1,state_h1,state_c1 = encoder_lstm1(enc_emb)\n",
        "encoder_lstm2 = LSTM(latent_dim,return_state=True,return_sequences=True)\n",
        "encoder_output2,state_h2,state_c2 = encoder_lstm2(encoder_output1)\n",
        "encoder_lstm3 = LSTM(latent_dim,return_state=True,return_sequences=True)\n",
        "encoder_output3,state_h3,state_c3 = encoder_lstm3(encoder_output2)\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(y_voc_size,latent_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "decoder_outputs,decoder_fwd_state,decoder_back_state=decoder_lstm(dec_emb,initial_state=[state_h3,state_c3])\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out,attn_states=attn_layer([encoder_output3,decoder_outputs])\n",
        "decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_outputs,attn_out])\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size,activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')\n",
        "es = EarlyStopping(monitor='val_loss',mode='min',verbose=1)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5wwtawRNhVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]],y_tr.reshape(y_tr.shape[0],y_tr.shape[1],1)[:,1:],epochs=50,callbacks=[es],batch_size=20,validation_data=([x_val,y_val[:,:-1]],y_val.reshape(y_val.shape[0],y_val.shape[1],1)[:,1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uKdL1no8iun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "fe1d5182-7816-405d-8f9f-86ff29ac94cc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}